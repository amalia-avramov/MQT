#se poate folosi docker-compose-ul din L2, cel cu 1 broker
#we can use docker-compose file from Mosule2 
1. Create topics

Create  topic events1 with 1 replication factor and 3 partitions.

/usr/bin/kafka-topics --create --bootstrap-server kafka:9092 localhost:9092 --topic events1 --partitions 3 --replication-factor 1

Create  topic events2 with 1 replication factor and 4 partitions.

/usr/bin/kafka-topics --create --bootstrap-server kafka:9092 localhost:9092 --topic events2 --partitions 4 --replication-factor 1

2. List all topics 

/usr/bin/kafka-topics --list --bootstrap-server kafka:9092

3. Describe topic 

/usr/bin/kafka-topics --bootstrap-server kafka:9092 localhost:9092 --describe --topic events1

/usr/bin/kafka-topics --bootstrap-server kafka:9092 localhost:9092 --describe --topic events2

4. Create a console producer foe topic event1. 

docker exec -ti kafka /usr/bin/kafka-console-producer --bootstrap-server kafka:9092 --topic events1

3. Read the data - create 2 consumers for event1. Show partition number and offset. 

4. Send data. Use your Producer API
SimpleExampleProducer
SynchronousSimpleProducer
AsynchronousSimpleProducer
ExampleProducer --> add arg[0] parameter
create group of Consumers 

5. Run with specifying consumer group and printing the partition
- from-beginning
-latest

Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic events2 --property print.key=true --property key.separator="," --property print.partition=true --property print.offset=true --group con1 --from-beginning

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic events2 --property print.key=true --property print.partition=true --property print.offset=true --group con1

6. Delete topic

/usr/bin/kafka-topics --bootstrap-server kafka:9092 —topic events1 —delete

